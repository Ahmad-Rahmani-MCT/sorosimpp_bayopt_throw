{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf35b72",
   "metadata": {},
   "source": [
    "neccessary imports, selecting the compute device, setting seed, data loading and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af7e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using compute device: cpu\n",
      "shape of the dataset:  (36380, 9)\n"
     ]
    }
   ],
   "source": [
    "# setting the seeds \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import random \n",
    "import torch \n",
    "import pandas as pd  \n",
    "\n",
    "##  user inputs ## \n",
    "# names and directories \n",
    "dataset_dir_name = \"datasets\" # directory to access the dataset and save the dataset plots\n",
    "dataset_filename = \"amprs_10hz_36k.csv\" # filename of the dataset \n",
    "data_plot_dir_name = \"amprs_10hz_36k_plots\" # directory name to save plots within dataset_dir_name\n",
    "network_data_dir_name = \"network_data\" # directory to save the scalers and networks weights and biases \n",
    "lines_dataset_filename = \"lines_10hz_300.csv\" # name of the lines dataset\n",
    "# determining the indices to reference the dataset  \n",
    "input_1_index = 0 \n",
    "input_2_index = 1 \n",
    "input_3_index = 2 \n",
    "midsection_x_index = 3 \n",
    "midsection_y_index = 4 \n",
    "midsection_z_index = 5 \n",
    "ee_x_index = 6 \n",
    "ee_y_index = 7 \n",
    "ee_z_index = 8 \n",
    "input_start_index = 0  \n",
    "input_stop_index = 3 \n",
    "state_start_index = 3 \n",
    "state_stop_index = 9  \n",
    "# train, validation and testing split \n",
    "train_percent = 0.7 \n",
    "valid_percent = 0.15 \n",
    "test_percent = 0.15  \n",
    "# state and input lags \n",
    "lag_input = 0 \n",
    "lag_state = 1   \n",
    "# nn configuration and training parameters \n",
    "num_hidden_layers = 0\n",
    "hidden_units = 30  \n",
    "learning_rate_autoregressive = 0.0001 \n",
    "epochs_autoregressive = 2000  \n",
    "model_name = \"MLP_AR\" \n",
    "es_patience = 6 \n",
    "chunk_length = 10 \n",
    "\n",
    "# selecting the compute device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"using compute device: {device}\") \n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")  \n",
    "\n",
    "# setting the seeds \n",
    "def set_all_seeds(seed: int = 42):\n",
    "    \"\"\"\n",
    "    sets the seeds for python, numpy, and pytoch (CPU & GPU) \n",
    "    \"\"\"\n",
    "    # python random module\n",
    "    random.seed(seed)\n",
    "    # numpy\n",
    "    np.random.seed(seed)\n",
    "    # pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    # pytorch (GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seeds set to {seed}\") \n",
    "\n",
    "# setting up directory paths and loading the dataset\n",
    "script_directory = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd() \n",
    "dataset_path = os.path.join(script_directory, dataset_dir_name) \n",
    "def load_data(file_path: str): \n",
    "    \"\"\"\n",
    "    Reads a csv and returns a numpy array of all the data \n",
    "    SETS THE INDEX TO THE TIME COLUMN\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path) \n",
    "    df.set_index('time',inplace=True)  \n",
    "    data = df.to_numpy() \n",
    "    return data   \n",
    "dataset = load_data(os.path.join(dataset_path, dataset_filename)) \n",
    "print(\"shape of the dataset: \", dataset.shape)  \n",
    "\n",
    "# data plot directory \n",
    "data_plot_path = os.path.join(dataset_path, data_plot_dir_name) \n",
    "os.makedirs(data_plot_path, exist_ok=True) \n",
    "\n",
    "# plotting  \n",
    "# 3d trajectory plot \n",
    "plot_name = \"x_y_z plot.png\"  \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(dataset[:, ee_x_index], dataset[:, ee_y_index], dataset[:, ee_z_index], s=5)\n",
    "ax.scatter(dataset[:, midsection_x_index], dataset[:, midsection_y_index], dataset[:, midsection_z_index], s=5)\n",
    "ax.set_zlim(-0.4, 0)\n",
    "ax.set_xlabel('EE_X[m]')\n",
    "ax.set_ylabel('EE_Y[m]')\n",
    "ax.set_zlabel('EE_Z[m]')\n",
    "ax.set_title('3D EE Trajectory')\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(data_plot_path, plot_name), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close() \n",
    "# 3d ee trajectory plot \n",
    "plot_name = \"ee_x_y_z plot.png\"  \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(dataset[:, ee_x_index], dataset[:, ee_y_index], dataset[:, ee_z_index], s=5)\n",
    "ax.set_zlim(-0.4, 0)\n",
    "ax.set_xlabel('EE_X[m]')\n",
    "ax.set_ylabel('EE_Y[m]')\n",
    "ax.set_zlabel('EE_Z[m]')\n",
    "ax.set_title('3D EE Trajectory')\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(data_plot_path, plot_name), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close() \n",
    "# 2d ee trajectory plot \n",
    "plot_name = \"ee_x_y_plot.png\"  \n",
    "plt.figure()\n",
    "plt.scatter(dataset[:,ee_x_index],dataset[:,ee_y_index], s=5)\n",
    "plt.xlabel('EE_x[m]') \n",
    "plt.ylabel('EE_y[m]') \n",
    "plt.title('2D EE Trajectory')  \n",
    "plt.tight_layout()  \n",
    "plt.savefig(os.path.join(data_plot_path, plot_name), dpi=300, bbox_inches=\"tight\") \n",
    "plt.close()  \n",
    "# actuator inputs plots \n",
    "for i in range(input_stop_index) : \n",
    "    plot_name = f\"u{i+1}_plot.png\"   \n",
    "    plt.figure()\n",
    "    plt.plot(dataset[:,i])  \n",
    "    plt.xlabel('Sample') \n",
    "    plt.ylabel(f\"Actuator {i+1} Value\") \n",
    "    plt.title(f\"Actuator {i+1} Inputs\")  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig(os.path.join(data_plot_path, plot_name), dpi=300, bbox_inches=\"tight\") \n",
    "    plt.close()  \n",
    "# state plots \n",
    "for i in range(state_start_index,state_stop_index) : \n",
    "    plot_name = f\"x_{i+1}_plot.png\"   \n",
    "    plt.figure()\n",
    "    plt.plot(dataset[:,i])  \n",
    "    plt.xlabel('Sample') \n",
    "    plt.ylabel(f\"State {i+1} Value\") \n",
    "    plt.title(f\"State {i+1} Values\")  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig(os.path.join(data_plot_path, plot_name), dpi=300, bbox_inches=\"tight\") \n",
    "    plt.close()  \n",
    "\n",
    "# small input plots\n",
    "for i in range(input_stop_index) : \n",
    "    plot_name = f\"u{i+1}_plot_limitted.png\"   \n",
    "    plt.figure()\n",
    "    plt.plot(dataset[:150,i])  \n",
    "    plt.xlabel('Sample') \n",
    "    plt.ylabel(f\"Actuator {i+1} Value\") \n",
    "    plt.title(f\"Actuator {i+1} Inputs\")  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig(os.path.join(data_plot_path, plot_name), dpi=300, bbox_inches=\"tight\") \n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c2453",
   "metadata": {},
   "source": [
    "dividing into input and states and train, validation and testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b589ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape:  (36380, 3)\n",
      "states shape:  (36380, 6)\n",
      "U and X train split shape:  (25466, 3) (25466, 6)\n",
      "U and X valid split shape:  (5457, 3) (5457, 6)\n",
      "U and X test split shape:  (5457, 3) (5457, 6)\n"
     ]
    }
   ],
   "source": [
    "U = dataset[:,input_start_index:input_stop_index] \n",
    "X = dataset[:,state_start_index:state_stop_index] \n",
    "print(\"inputs shape: \", U.shape) \n",
    "print(\"states shape: \", X.shape)  \n",
    "\n",
    "# train, validation and testing split \n",
    "def train_valid_test_split(dataset: np.array, train_percent: int, valid_percent: int) : \n",
    "    \"\"\"\n",
    "    splits a dataset into training, validation and testing sets\n",
    "    \"\"\"\n",
    "    len_data = len(dataset) \n",
    "    train_size = int(train_percent*len_data) \n",
    "    valid_size = int(valid_percent*len_data) \n",
    "    train_data = dataset[0:train_size] \n",
    "    valid_data = dataset[train_size:train_size+valid_size] \n",
    "    test_data = dataset[train_size+valid_size:] \n",
    "    return train_data, valid_data, test_data  \n",
    "U_train, U_valid, U_test = train_valid_test_split(dataset=U, train_percent=train_percent, valid_percent=valid_percent) \n",
    "X_train, X_valid, X_test = train_valid_test_split(dataset=X, train_percent=train_percent, valid_percent=valid_percent) \n",
    "print(\"U and X train split shape: \", U_train.shape, X_train.shape) \n",
    "print(\"U and X valid split shape: \", U_valid.shape, X_valid.shape) \n",
    "print(\"U and X test split shape: \", U_test.shape, X_test.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c9142",
   "metadata": {},
   "source": [
    "scaling the datasets and saving the scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "input_scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "state_scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "input_scaler.fit(U_train)   \n",
    "state_scaler.fit(X_train) \n",
    "U_train_scaled = input_scaler.transform(U_train) \n",
    "U_valid_scaled = input_scaler.transform(U_valid) \n",
    "U_test_scaled = input_scaler.transform(U_test)  \n",
    "X_train_scaled = state_scaler.transform(X_train) \n",
    "X_valid_scaled = state_scaler.transform(X_valid) \n",
    "X_test_scaled = state_scaler.transform(X_test)  \n",
    "# saving the scalers  \n",
    "network_data_dir_path = os.path.join(script_directory, network_data_dir_name) \n",
    "os.makedirs(network_data_dir_path, exist_ok=True)\n",
    "import pickle\n",
    "input_scaler_filename = \"input_scaler.pkl\" \n",
    "state_scaler_filename = \"state_scaler.pkl\" \n",
    "with open(os.path.join(network_data_dir_path, input_scaler_filename), \"wb\") as file : \n",
    "    pickle.dump(input_scaler, file=file)\n",
    "with open(os.path.join(network_data_dir_path, state_scaler_filename), \"wb\") as file : \n",
    "    pickle.dump(state_scaler, file=file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaec20",
   "metadata": {},
   "source": [
    "creating features and labels (USED FOR TF TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4b238a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the training features and labels:  (25464, 15) (25464, 6)\n",
      "shape of the validation features and labels:  (5455, 15) (5455, 6)\n",
      "shape of the testing features and labels:  (5455, 15) (5455, 6)\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(U: np.array, X: np.array, lag_input: int, lag_state: int) :  \n",
    "    \"\"\"\n",
    "    creates features and labels based on the state and input lag for a forward dynamical model \n",
    "    assumes the following x_k+1 = f(x_k,x_k-1:nx, u_k, u_k-1:nu)\n",
    "    \"\"\"\n",
    "    \n",
    "    features = [] \n",
    "    labels = [] \n",
    "\n",
    "    max_lag = max(lag_input, lag_state)\n",
    "\n",
    "    for i in range(max_lag, len(U)-1) : \n",
    "        current_state = X[i,:] \n",
    "        if lag_state == 0 : \n",
    "            past_states = X[i:i] \n",
    "        else : \n",
    "            past_states = X[i-lag_state:i,:] \n",
    "            past_states = past_states.flatten('C') \n",
    "        current_input = U[i,:] \n",
    "        if lag_input == 0 : \n",
    "            past_inputs = U[i:i] \n",
    "        else : \n",
    "            past_inputs = U[i-lag_input:i,:] \n",
    "            past_inputs = past_inputs.flatten('C')\n",
    "        \n",
    "        if past_states.size == 0 and past_inputs.size == 0 : \n",
    "            joined_features = np.concatenate((current_state, current_input), axis=0) \n",
    "        elif past_states.size != 0 and past_inputs.size == 0 : \n",
    "            joined_features = np.concatenate((current_state, past_states, current_input), axis=0)\n",
    "        elif past_states.size == 0 and past_inputs.size != 0 : \n",
    "            joined_features = np.concatenate((current_state, current_input, past_inputs), axis=0) \n",
    "        else : \n",
    "            joined_features = np.concatenate((current_state, past_states, current_input, past_inputs), axis=0) \n",
    "\n",
    "        features.append(joined_features) \n",
    "        labels.append(X[i+1]) \n",
    "    \n",
    "    features = np.array(features) \n",
    "    labels = np.array(labels)  \n",
    "\n",
    "    return features, labels \n",
    "\n",
    "train_features, train_labels = prepare_dataset(U=U_train_scaled, X=X_train_scaled, lag_input=lag_input, lag_state=lag_state) \n",
    "print(\"shape of the training features and labels: \", train_features.shape, train_labels.shape) \n",
    "valid_features, valid_labels = prepare_dataset(U=U_valid_scaled, X=X_valid_scaled, lag_input=lag_input, lag_state=lag_state) \n",
    "print(\"shape of the validation features and labels: \", valid_features.shape, valid_labels.shape) \n",
    "test_features, test_labels = prepare_dataset(U=U_test_scaled, X=X_test_scaled, lag_input=lag_input, lag_state=lag_state) \n",
    "print(\"shape of the testing features and labels: \", test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc53049b",
   "metadata": {},
   "source": [
    "conversion to tensor (torch.float32) and creation of dataloaders (for TF testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10243f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training feature and label tensor shape and datatype:  torch.Size([25464, 15]) torch.Size([25464, 6]) torch.float32\n",
      "validation feature and label tensor shape and datatype:  torch.Size([5455, 15]) torch.Size([5455, 6]) torch.float32\n",
      "testing feature and label tensor shape and datatype:  torch.Size([5455, 15]) torch.Size([5455, 6]) torch.float32\n",
      "train dataloader length:  1592\n",
      "valid dataloader length:  341\n",
      "test dataloader length:  341\n",
      "shape of a single batch feature and label:  torch.Size([16, 15]) torch.Size([16, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "train_features_tensor = torch.from_numpy(train_features).type(dtype=torch.float32) \n",
    "train_labels_tensor = torch.from_numpy(train_labels).type(dtype=torch.float32) \n",
    "valid_features_tensor = torch.from_numpy(valid_features).type(dtype=torch.float32) \n",
    "valid_labels_tensor = torch.from_numpy(valid_labels).type(dtype=torch.float32) \n",
    "test_features_tensor = torch.from_numpy(test_features).type(dtype=torch.float32) \n",
    "test_labels_tensor = torch.from_numpy(test_labels).type(dtype=torch.float32) \n",
    "\n",
    "print(\"training feature and label tensor shape and datatype: \", train_features_tensor.shape, train_labels_tensor.shape, train_labels_tensor.dtype)\n",
    "print(\"validation feature and label tensor shape and datatype: \", valid_features_tensor.shape, valid_labels_tensor.shape, valid_labels_tensor.dtype)\n",
    "print(\"testing feature and label tensor shape and datatype: \", test_features_tensor.shape, test_labels_tensor.shape, test_labels_tensor.dtype) \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor) \n",
    "valid_dataset = TensorDataset(valid_features_tensor, valid_labels_tensor)\n",
    "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor) \n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size) \n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size) \n",
    "\n",
    "print(\"train dataloader length: \", len(train_dataloader)) \n",
    "print(\"valid dataloader length: \", len(valid_dataloader)) \n",
    "print(\"test dataloader length: \", len(test_dataloader)) \n",
    "\n",
    "for X,Y in train_dataloader : \n",
    "    print(\"shape of a single batch feature and label: \", X.shape, Y.shape)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca6d96",
   "metadata": {},
   "source": [
    "defining and creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff60cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLP_model                                [1, 6]                    --\n",
      "├─Linear: 1-1                            [1, 30]                   480\n",
      "├─ReLU: 1-2                              [1, 30]                   --\n",
      "├─Sequential: 1-3                        [1, 30]                   --\n",
      "├─Linear: 1-4                            [1, 6]                    186\n",
      "==========================================================================================\n",
      "Total params: 666\n",
      "Trainable params: 666\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "class MLP_model(torch.nn.Module): \n",
    "    def __init__(self, input_flat_size:int, hidden_units:int, output_size:int, num_hidden_layers:int) :\n",
    "        super().__init__()\n",
    "        self.input_flat_size = input_flat_size \n",
    "        self.hidden_units = hidden_units \n",
    "        self.output_size = output_size \n",
    "        self.num_hidden_layers = num_hidden_layers \n",
    "\n",
    "        hidden_layers = [] \n",
    "\n",
    "        in_dimension = self.input_flat_size \n",
    "\n",
    "        self.input_layer = torch.nn.Linear(in_features=in_dimension, out_features=self.hidden_units) \n",
    "        \n",
    "        for i in range(self.num_hidden_layers) : \n",
    "            hidden_layers.append(torch.nn.Linear(in_features=self.hidden_units, out_features=self.hidden_units)) \n",
    "            hidden_layers.append(torch.nn.ReLU()) \n",
    "\n",
    "        self.backbone = torch.nn.Sequential(*hidden_layers) \n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(in_features=self.hidden_units, out_features=self.output_size) \n",
    " \n",
    "        self.relu = torch.nn.ReLU()    \n",
    "\n",
    "    def forward(self,x): \n",
    "        out = self.input_layer(x) \n",
    "        out = self.relu(out)\n",
    "        out = self.backbone(out)  \n",
    "        out = self.output_layer(out) \n",
    "        return out \n",
    "    \n",
    "\n",
    "\n",
    "input_flat_size = train_features.shape[1]\n",
    "output_size = train_labels.shape[1] \n",
    "\n",
    "forward_model = MLP_model(input_flat_size=input_flat_size, hidden_units=hidden_units, output_size=output_size, num_hidden_layers=num_hidden_layers) \n",
    "\n",
    "from torchinfo import summary \n",
    "print(summary(model=forward_model, input_size=(1,input_flat_size)))\n",
    "\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "\n",
    "optimizer_autoregressive = torch.optim.Adam(forward_model.parameters(), lr=learning_rate_autoregressive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b34d6",
   "metadata": {},
   "source": [
    "Defining the autoregressive training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a62b0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoregressive(model: torch.nn.Module, U_scaled_train: np.array, X_scaled_train: np.array, \n",
    "                         U_scaled_valid: np.array, X_scaled_valid: np.array, \n",
    "                         lag_input: int, lag_state: int, \n",
    "                         epochs: int, optimizer: torch.optim.Optimizer, loss_fn: torch.nn.Module, \n",
    "                         model_name: str, chunk_length: int, early_stopping_call = None):   \n",
    "    \n",
    "    results = {\"train_loss\": [], \"valid_loss\": []}\n",
    "\n",
    "    U_train = torch.from_numpy(U_scaled_train).type(torch.float32)\n",
    "    X_train = torch.from_numpy(X_scaled_train).type(torch.float32)\n",
    "    U_valid = torch.from_numpy(U_scaled_valid).type(torch.float32)\n",
    "    X_valid = torch.from_numpy(X_scaled_valid).type(torch.float32)  \n",
    "\n",
    "    max_lag = max(lag_input, lag_state)\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        X_train_buffer = X_train.clone()\n",
    "        X_valid_buffer = X_valid.clone()\n",
    "\n",
    "        model.train() \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        preds = []  \n",
    "        labels = [] \n",
    "\n",
    "        total_loss = 0 \n",
    "        step = 0  \n",
    "        counter = 0\n",
    "\n",
    "        # initial buffer filled with ground truth \n",
    "        current_state = X_train[max_lag,:] \n",
    "        if lag_state == 0 : \n",
    "            past_state = X_train[max_lag:max_lag] \n",
    "        else : \n",
    "            past_state = X_train[max_lag-lag_state:max_lag,:] \n",
    "            past_state = torch.flatten(input=past_state) \n",
    "        current_input = U_train[max_lag,:] \n",
    "        if lag_input == 0 : \n",
    "            past_input = U_train[max_lag:max_lag,:]\n",
    "        else : \n",
    "            past_input = U_train[max_lag-lag_input:max_lag,:] \n",
    "            past_input = torch.flatten(input=past_input) \n",
    "\n",
    "        if past_state.size(dim=0) == 0 and past_input.size(dim=0) == 0 : \n",
    "            joined_features = torch.concatenate((current_state, current_input), dim=0) \n",
    "        elif past_state.size(dim=0) != 0 and past_input.size(dim=0) == 0 : \n",
    "            joined_features = torch.concatenate((current_state, past_state, current_input), dim=0)\n",
    "        elif past_state.size(dim=0) == 0 and past_input.size(dim=0) != 0 : \n",
    "            joined_features = torch.concatenate((current_state, current_input, past_input), dim=0) \n",
    "        else : \n",
    "            joined_features = torch.concatenate((current_state, past_state, current_input, past_input), dim=0)  \n",
    "\n",
    "        for i in range(max_lag+1, len(U_train)) : \n",
    "\n",
    "            pred = model(joined_features.unsqueeze(0)) \n",
    "            pred = pred.squeeze(0) \n",
    "\n",
    "            step += 1 \n",
    "\n",
    "            preds.append(pred)  \n",
    "\n",
    "            labels.append(X_train[i])\n",
    "\n",
    "            X_train_buffer[i,:] = pred  \n",
    "\n",
    "            if i < len(U_train) - 1 :\n",
    "\n",
    "                # buffer update \n",
    "                current_state = pred\n",
    "                if lag_state == 0 : \n",
    "                    past_state = X_train_buffer[i:i] \n",
    "                else : \n",
    "                    past_state = X_train_buffer[i-lag_state:i,:] \n",
    "                    past_state = torch.flatten(input=past_state)\n",
    "                current_input = U_train[i,:] \n",
    "                if lag_input == 0 : \n",
    "                    past_input = U_train[i:i,:]\n",
    "                else : \n",
    "                    past_input = U_train[i-lag_input:i,:] \n",
    "                    past_input = torch.flatten(input=past_input) \n",
    "\n",
    "                if past_state.size(dim=0) == 0 and past_input.size(dim=0) == 0 : \n",
    "                    joined_features = torch.concatenate((current_state, current_input), dim=0) \n",
    "                elif past_state.size(dim=0) != 0 and past_input.size(dim=0) == 0 : \n",
    "                    joined_features = torch.concatenate((current_state, past_state, current_input), dim=0)\n",
    "                elif past_state.size(dim=0) == 0 and past_input.size(dim=0) != 0 : \n",
    "                    joined_features = torch.concatenate((current_state, current_input, past_input), dim=0) \n",
    "                else : \n",
    "                    joined_features = torch.concatenate((current_state, past_state, current_input, past_input), dim=0) \n",
    "\n",
    "            else : \n",
    "                pass  \n",
    "\n",
    "            if step == chunk_length or i == len(U_train) - 1:  \n",
    "\n",
    "                preds_tensor = torch.stack(preds, dim=0)  \n",
    "\n",
    "                labels_tensor = torch.stack(labels, dim=0)\n",
    "\n",
    "                loss = loss_fn(preds_tensor, labels_tensor)  \n",
    "\n",
    "                loss.backward() \n",
    "                total_loss += loss.item()\n",
    "                optimizer.step()  \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                current_state = current_state.detach() \n",
    "                X_train_buffer = X_train_buffer.detach() \n",
    "                joined_features = joined_features.detach()\n",
    "\n",
    "                total_loss += loss.item() \n",
    "                \n",
    "                step = 0 \n",
    "                preds = [] \n",
    "                labels = []   \n",
    "                counter += 1  \n",
    "            else : \n",
    "                pass\n",
    "\n",
    "        train_loss = total_loss/counter\n",
    "\n",
    "        # validation \n",
    "        model.eval() \n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # initial buffer filled with ground truth \n",
    "        current_state = X_valid[max_lag,:] \n",
    "        if lag_state == 0 : \n",
    "            past_state = X_valid[max_lag:max_lag] \n",
    "        else : \n",
    "            past_state = X_valid[max_lag-lag_state:max_lag,:] \n",
    "            past_state = torch.flatten(input=past_state) \n",
    "        current_input = U_valid[max_lag,:] \n",
    "        if lag_input == 0 : \n",
    "            past_input = U_valid[max_lag:max_lag,:]\n",
    "        else : \n",
    "            past_input = U_valid[max_lag-lag_input:max_lag,:] \n",
    "            past_input = torch.flatten(input=past_input) \n",
    "\n",
    "        if past_state.size(dim=0) == 0 and past_input.size(dim=0) == 0 : \n",
    "            joined_features = torch.concatenate((current_state, current_input), dim=0) \n",
    "        elif past_state.size(dim=0) != 0 and past_input.size(dim=0) == 0 : \n",
    "            joined_features = torch.concatenate((current_state, past_state, current_input), dim=0)\n",
    "        elif past_state.size(dim=0) == 0 and past_input.size(dim=0) != 0 : \n",
    "            joined_features = torch.concatenate((current_state, current_input, past_input), dim=0) \n",
    "        else : \n",
    "            joined_features = torch.concatenate((current_state, past_state, current_input, past_input), dim=0) \n",
    "        \n",
    "        with torch.inference_mode() :  \n",
    "\n",
    "            for i in range(max_lag+1, len(U_valid)) : \n",
    "\n",
    "                pred = model(joined_features.unsqueeze(0)) \n",
    "                pred = pred.squeeze(0) \n",
    "\n",
    "                preds.append(pred) \n",
    "\n",
    "                X_valid_buffer[i,:] = pred  \n",
    "\n",
    "                if i < len(U_valid) - 1 : \n",
    "\n",
    "                    # buffer update \n",
    "                    current_state = pred\n",
    "                    if lag_state == 0 : \n",
    "                        past_state = X_valid_buffer[i:i] \n",
    "                    else : \n",
    "                        past_state = X_valid_buffer[i-lag_state:i,:] \n",
    "                        past_state = torch.flatten(input=past_state)\n",
    "                    current_input = U_valid[i,:] \n",
    "                    if lag_input == 0 : \n",
    "                        past_input = U_valid[i:i,:]\n",
    "                    else : \n",
    "                        past_input = U_valid[i-lag_input:i,:] \n",
    "                        past_input = torch.flatten(input=past_input)  \n",
    "\n",
    "                    if past_state.size(dim=0) == 0 and past_input.size(dim=0) == 0 : \n",
    "                        joined_features = torch.concatenate((current_state, current_input), dim=0) \n",
    "                    elif past_state.size(dim=0) != 0 and past_input.size(dim=0) == 0 : \n",
    "                        joined_features = torch.concatenate((current_state, past_state, current_input), dim=0)\n",
    "                    elif past_state.size(dim=0) == 0 and past_input.size(dim=0) != 0 : \n",
    "                        joined_features = torch.concatenate((current_state, current_input, past_input), dim=0) \n",
    "                    else : \n",
    "                        joined_features = torch.concatenate((current_state, past_state, current_input, past_input), dim=0)\n",
    "                     \n",
    "\n",
    "                else : \n",
    "                    pass\n",
    "\n",
    "        preds_tensor = torch.stack(preds, dim=0) \n",
    "\n",
    "        targets = X_valid[max_lag+1:] \n",
    "\n",
    "        loss = loss_fn(preds_tensor, targets) \n",
    "\n",
    "        valid_loss = loss.item() \n",
    "\n",
    "        print(\n",
    "        f\"Model: {model_name} |\"\n",
    "        f\"Epoch: {epoch + 1} | \"\n",
    "        f\"Train Loss: {train_loss:.6f} | \" \n",
    "        f\"Validation_Loss: {valid_loss:.6f} | \" \n",
    "        ) \n",
    "\n",
    "        results[\"train_loss\"].append(train_loss) \n",
    "        results[\"valid_loss\"].append(valid_loss)\n",
    "\n",
    "        if early_stopping_call is not None: \n",
    "            early_stopping_call(valid_loss) \n",
    "            if early_stopping_call.early_stop : \n",
    "                print(\"Early Stopping Triggered\") \n",
    "                break \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a0d1d",
   "metadata": {},
   "source": [
    "defining early stopping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3785ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=6, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        patience: number of epochs to wait for improvement\n",
    "        min_delta: minimum improvement in validation loss to count as progress\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf \n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31616e9a",
   "metadata": {},
   "source": [
    "training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fce37b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MLP_AR |Epoch: 1 | Train Loss: 0.078289 | Validation_Loss: 0.006682 | \n",
      "Model: MLP_AR |Epoch: 2 | Train Loss: 0.010597 | Validation_Loss: 0.004174 | \n",
      "Model: MLP_AR |Epoch: 3 | Train Loss: 0.008208 | Validation_Loss: 0.003572 | \n",
      "Model: MLP_AR |Epoch: 4 | Train Loss: 0.007338 | Validation_Loss: 0.003235 | \n",
      "Model: MLP_AR |Epoch: 5 | Train Loss: 0.006686 | Validation_Loss: 0.002945 | \n",
      "Model: MLP_AR |Epoch: 6 | Train Loss: 0.006087 | Validation_Loss: 0.002678 | \n",
      "Model: MLP_AR |Epoch: 7 | Train Loss: 0.005542 | Validation_Loss: 0.002443 | \n",
      "Model: MLP_AR |Epoch: 8 | Train Loss: 0.005069 | Validation_Loss: 0.002252 | \n",
      "Model: MLP_AR |Epoch: 9 | Train Loss: 0.004677 | Validation_Loss: 0.002101 | \n",
      "Model: MLP_AR |Epoch: 10 | Train Loss: 0.004359 | Validation_Loss: 0.001980 | \n",
      "Model: MLP_AR |Epoch: 11 | Train Loss: 0.004100 | Validation_Loss: 0.001880 | \n",
      "Model: MLP_AR |Epoch: 12 | Train Loss: 0.003883 | Validation_Loss: 0.001790 | \n",
      "Model: MLP_AR |Epoch: 13 | Train Loss: 0.003695 | Validation_Loss: 0.001711 | \n",
      "Model: MLP_AR |Epoch: 14 | Train Loss: 0.003530 | Validation_Loss: 0.001642 | \n",
      "Model: MLP_AR |Epoch: 15 | Train Loss: 0.003385 | Validation_Loss: 0.001582 | \n",
      "Model: MLP_AR |Epoch: 16 | Train Loss: 0.003255 | Validation_Loss: 0.001529 | \n",
      "Model: MLP_AR |Epoch: 17 | Train Loss: 0.003138 | Validation_Loss: 0.001484 | \n",
      "Model: MLP_AR |Epoch: 18 | Train Loss: 0.003033 | Validation_Loss: 0.001442 | \n",
      "Model: MLP_AR |Epoch: 19 | Train Loss: 0.002938 | Validation_Loss: 0.001404 | \n",
      "Model: MLP_AR |Epoch: 20 | Train Loss: 0.002853 | Validation_Loss: 0.001371 | \n",
      "Model: MLP_AR |Epoch: 21 | Train Loss: 0.002780 | Validation_Loss: 0.001343 | \n",
      "Model: MLP_AR |Epoch: 22 | Train Loss: 0.002718 | Validation_Loss: 0.001318 | \n",
      "Model: MLP_AR |Epoch: 23 | Train Loss: 0.002664 | Validation_Loss: 0.001297 | \n",
      "Model: MLP_AR |Epoch: 24 | Train Loss: 0.002618 | Validation_Loss: 0.001279 | \n",
      "Model: MLP_AR |Epoch: 25 | Train Loss: 0.002578 | Validation_Loss: 0.001265 | \n",
      "Model: MLP_AR |Epoch: 26 | Train Loss: 0.002543 | Validation_Loss: 0.001252 | \n",
      "Model: MLP_AR |Epoch: 27 | Train Loss: 0.002513 | Validation_Loss: 0.001240 | \n",
      "Model: MLP_AR |Epoch: 28 | Train Loss: 0.002485 | Validation_Loss: 0.001229 | \n",
      "Model: MLP_AR |Epoch: 29 | Train Loss: 0.002460 | Validation_Loss: 0.001219 | \n",
      "Model: MLP_AR |Epoch: 30 | Train Loss: 0.002437 | Validation_Loss: 0.001210 | \n",
      "Model: MLP_AR |Epoch: 31 | Train Loss: 0.002416 | Validation_Loss: 0.001201 | \n",
      "Model: MLP_AR |Epoch: 32 | Train Loss: 0.002396 | Validation_Loss: 0.001193 | \n",
      "Model: MLP_AR |Epoch: 33 | Train Loss: 0.002377 | Validation_Loss: 0.001184 | \n",
      "Model: MLP_AR |Epoch: 34 | Train Loss: 0.002360 | Validation_Loss: 0.001177 | \n",
      "Model: MLP_AR |Epoch: 35 | Train Loss: 0.002344 | Validation_Loss: 0.001170 | \n",
      "Model: MLP_AR |Epoch: 36 | Train Loss: 0.002329 | Validation_Loss: 0.001163 | \n",
      "Model: MLP_AR |Epoch: 37 | Train Loss: 0.002314 | Validation_Loss: 0.001158 | \n",
      "Model: MLP_AR |Epoch: 38 | Train Loss: 0.002301 | Validation_Loss: 0.001154 | \n",
      "Model: MLP_AR |Epoch: 39 | Train Loss: 0.002289 | Validation_Loss: 0.001147 | \n",
      "Model: MLP_AR |Epoch: 40 | Train Loss: 0.002276 | Validation_Loss: 0.001145 | \n",
      "Model: MLP_AR |Epoch: 41 | Train Loss: 0.002265 | Validation_Loss: 0.001140 | \n",
      "Model: MLP_AR |Epoch: 42 | Train Loss: 0.002254 | Validation_Loss: 0.001132 | \n",
      "Model: MLP_AR |Epoch: 43 | Train Loss: 0.002242 | Validation_Loss: 0.001132 | \n",
      "Model: MLP_AR |Epoch: 44 | Train Loss: 0.002231 | Validation_Loss: 0.001129 | \n",
      "Model: MLP_AR |Epoch: 45 | Train Loss: 0.002221 | Validation_Loss: 0.001125 | \n",
      "Model: MLP_AR |Epoch: 46 | Train Loss: 0.002211 | Validation_Loss: 0.001121 | \n",
      "Model: MLP_AR |Epoch: 47 | Train Loss: 0.002202 | Validation_Loss: 0.001118 | \n",
      "Model: MLP_AR |Epoch: 48 | Train Loss: 0.002194 | Validation_Loss: 0.001115 | \n",
      "Model: MLP_AR |Epoch: 49 | Train Loss: 0.002185 | Validation_Loss: 0.001111 | \n",
      "Model: MLP_AR |Epoch: 50 | Train Loss: 0.002176 | Validation_Loss: 0.001107 | \n",
      "Model: MLP_AR |Epoch: 51 | Train Loss: 0.002167 | Validation_Loss: 0.001105 | \n",
      "Model: MLP_AR |Epoch: 52 | Train Loss: 0.002158 | Validation_Loss: 0.001101 | \n",
      "Model: MLP_AR |Epoch: 53 | Train Loss: 0.002151 | Validation_Loss: 0.001098 | \n",
      "Model: MLP_AR |Epoch: 54 | Train Loss: 0.002143 | Validation_Loss: 0.001096 | \n",
      "Model: MLP_AR |Epoch: 55 | Train Loss: 0.002136 | Validation_Loss: 0.001094 | \n",
      "Model: MLP_AR |Epoch: 56 | Train Loss: 0.002129 | Validation_Loss: 0.001091 | \n",
      "Model: MLP_AR |Epoch: 57 | Train Loss: 0.002122 | Validation_Loss: 0.001087 | \n",
      "Model: MLP_AR |Epoch: 58 | Train Loss: 0.002114 | Validation_Loss: 0.001084 | \n",
      "Model: MLP_AR |Epoch: 59 | Train Loss: 0.002108 | Validation_Loss: 0.001082 | \n",
      "Model: MLP_AR |Epoch: 60 | Train Loss: 0.002101 | Validation_Loss: 0.001079 | \n",
      "Model: MLP_AR |Epoch: 61 | Train Loss: 0.002095 | Validation_Loss: 0.001076 | \n",
      "Model: MLP_AR |Epoch: 62 | Train Loss: 0.002089 | Validation_Loss: 0.001076 | \n",
      "Model: MLP_AR |Epoch: 63 | Train Loss: 0.002083 | Validation_Loss: 0.001074 | \n",
      "Model: MLP_AR |Epoch: 64 | Train Loss: 0.002077 | Validation_Loss: 0.001071 | \n",
      "Model: MLP_AR |Epoch: 65 | Train Loss: 0.002071 | Validation_Loss: 0.001069 | \n",
      "Model: MLP_AR |Epoch: 66 | Train Loss: 0.002066 | Validation_Loss: 0.001064 | \n",
      "Model: MLP_AR |Epoch: 67 | Train Loss: 0.002060 | Validation_Loss: 0.001060 | \n",
      "Model: MLP_AR |Epoch: 68 | Train Loss: 0.002054 | Validation_Loss: 0.001057 | \n",
      "Model: MLP_AR |Epoch: 69 | Train Loss: 0.002049 | Validation_Loss: 0.001055 | \n",
      "Model: MLP_AR |Epoch: 70 | Train Loss: 0.002044 | Validation_Loss: 0.001051 | \n",
      "Model: MLP_AR |Epoch: 71 | Train Loss: 0.002039 | Validation_Loss: 0.001048 | \n",
      "Model: MLP_AR |Epoch: 72 | Train Loss: 0.002034 | Validation_Loss: 0.001046 | \n",
      "Model: MLP_AR |Epoch: 73 | Train Loss: 0.002029 | Validation_Loss: 0.001043 | \n",
      "Model: MLP_AR |Epoch: 74 | Train Loss: 0.002024 | Validation_Loss: 0.001041 | \n",
      "Model: MLP_AR |Epoch: 75 | Train Loss: 0.002019 | Validation_Loss: 0.001039 | \n",
      "Model: MLP_AR |Epoch: 76 | Train Loss: 0.002015 | Validation_Loss: 0.001036 | \n",
      "Model: MLP_AR |Epoch: 77 | Train Loss: 0.002010 | Validation_Loss: 0.001035 | \n",
      "Model: MLP_AR |Epoch: 78 | Train Loss: 0.002005 | Validation_Loss: 0.001032 | \n",
      "Model: MLP_AR |Epoch: 79 | Train Loss: 0.002001 | Validation_Loss: 0.001031 | \n",
      "Model: MLP_AR |Epoch: 80 | Train Loss: 0.001997 | Validation_Loss: 0.001028 | \n",
      "Model: MLP_AR |Epoch: 81 | Train Loss: 0.001993 | Validation_Loss: 0.001026 | \n",
      "Model: MLP_AR |Epoch: 82 | Train Loss: 0.001989 | Validation_Loss: 0.001026 | \n",
      "Model: MLP_AR |Epoch: 83 | Train Loss: 0.001985 | Validation_Loss: 0.001024 | \n",
      "Model: MLP_AR |Epoch: 84 | Train Loss: 0.001981 | Validation_Loss: 0.001022 | \n",
      "Model: MLP_AR |Epoch: 85 | Train Loss: 0.001977 | Validation_Loss: 0.001021 | \n",
      "Model: MLP_AR |Epoch: 86 | Train Loss: 0.001973 | Validation_Loss: 0.001018 | \n",
      "Model: MLP_AR |Epoch: 87 | Train Loss: 0.001969 | Validation_Loss: 0.001016 | \n",
      "Model: MLP_AR |Epoch: 88 | Train Loss: 0.001966 | Validation_Loss: 0.001015 | \n",
      "Model: MLP_AR |Epoch: 89 | Train Loss: 0.001962 | Validation_Loss: 0.001013 | \n",
      "Model: MLP_AR |Epoch: 90 | Train Loss: 0.001959 | Validation_Loss: 0.001011 | \n",
      "Model: MLP_AR |Epoch: 91 | Train Loss: 0.001955 | Validation_Loss: 0.001009 | \n",
      "Model: MLP_AR |Epoch: 92 | Train Loss: 0.001952 | Validation_Loss: 0.001007 | \n",
      "Model: MLP_AR |Epoch: 93 | Train Loss: 0.001949 | Validation_Loss: 0.001006 | \n",
      "Model: MLP_AR |Epoch: 94 | Train Loss: 0.001946 | Validation_Loss: 0.001004 | \n",
      "Model: MLP_AR |Epoch: 95 | Train Loss: 0.001942 | Validation_Loss: 0.001003 | \n",
      "Model: MLP_AR |Epoch: 96 | Train Loss: 0.001939 | Validation_Loss: 0.001001 | \n",
      "Model: MLP_AR |Epoch: 97 | Train Loss: 0.001936 | Validation_Loss: 0.001000 | \n",
      "Model: MLP_AR |Epoch: 98 | Train Loss: 0.001933 | Validation_Loss: 0.000998 | \n",
      "Model: MLP_AR |Epoch: 99 | Train Loss: 0.001930 | Validation_Loss: 0.000997 | \n",
      "Model: MLP_AR |Epoch: 100 | Train Loss: 0.001928 | Validation_Loss: 0.000996 | \n",
      "Model: MLP_AR |Epoch: 101 | Train Loss: 0.001924 | Validation_Loss: 0.000994 | \n",
      "Model: MLP_AR |Epoch: 102 | Train Loss: 0.001922 | Validation_Loss: 0.000992 | \n",
      "Model: MLP_AR |Epoch: 103 | Train Loss: 0.001918 | Validation_Loss: 0.000991 | \n",
      "Model: MLP_AR |Epoch: 104 | Train Loss: 0.001916 | Validation_Loss: 0.000990 | \n",
      "Model: MLP_AR |Epoch: 105 | Train Loss: 0.001913 | Validation_Loss: 0.000988 | \n",
      "Model: MLP_AR |Epoch: 106 | Train Loss: 0.001911 | Validation_Loss: 0.000987 | \n",
      "Model: MLP_AR |Epoch: 107 | Train Loss: 0.001908 | Validation_Loss: 0.000985 | \n",
      "Model: MLP_AR |Epoch: 108 | Train Loss: 0.001906 | Validation_Loss: 0.000984 | \n",
      "Model: MLP_AR |Epoch: 109 | Train Loss: 0.001903 | Validation_Loss: 0.000982 | \n",
      "Model: MLP_AR |Epoch: 110 | Train Loss: 0.001901 | Validation_Loss: 0.000981 | \n",
      "Model: MLP_AR |Epoch: 111 | Train Loss: 0.001899 | Validation_Loss: 0.000980 | \n",
      "Model: MLP_AR |Epoch: 112 | Train Loss: 0.001896 | Validation_Loss: 0.000978 | \n",
      "Model: MLP_AR |Epoch: 113 | Train Loss: 0.001894 | Validation_Loss: 0.000978 | \n",
      "Model: MLP_AR |Epoch: 114 | Train Loss: 0.001891 | Validation_Loss: 0.000977 | \n",
      "Model: MLP_AR |Epoch: 115 | Train Loss: 0.001889 | Validation_Loss: 0.000976 | \n",
      "Model: MLP_AR |Epoch: 116 | Train Loss: 0.001887 | Validation_Loss: 0.000973 | \n",
      "Model: MLP_AR |Epoch: 117 | Train Loss: 0.001884 | Validation_Loss: 0.000973 | \n",
      "Model: MLP_AR |Epoch: 118 | Train Loss: 0.001882 | Validation_Loss: 0.000972 | \n",
      "Model: MLP_AR |Epoch: 119 | Train Loss: 0.001880 | Validation_Loss: 0.000969 | \n",
      "Model: MLP_AR |Epoch: 120 | Train Loss: 0.001878 | Validation_Loss: 0.000968 | \n",
      "Model: MLP_AR |Epoch: 121 | Train Loss: 0.001875 | Validation_Loss: 0.000967 | \n",
      "Model: MLP_AR |Epoch: 122 | Train Loss: 0.001873 | Validation_Loss: 0.000966 | \n",
      "Model: MLP_AR |Epoch: 123 | Train Loss: 0.001870 | Validation_Loss: 0.000964 | \n",
      "Model: MLP_AR |Epoch: 124 | Train Loss: 0.001868 | Validation_Loss: 0.000963 | \n",
      "Model: MLP_AR |Epoch: 125 | Train Loss: 0.001865 | Validation_Loss: 0.000962 | \n",
      "Model: MLP_AR |Epoch: 126 | Train Loss: 0.001863 | Validation_Loss: 0.000960 | \n",
      "Model: MLP_AR |Epoch: 127 | Train Loss: 0.001861 | Validation_Loss: 0.000961 | \n",
      "Model: MLP_AR |Epoch: 128 | Train Loss: 0.001858 | Validation_Loss: 0.000960 | \n",
      "Model: MLP_AR |Epoch: 129 | Train Loss: 0.001856 | Validation_Loss: 0.000958 | \n",
      "Model: MLP_AR |Epoch: 130 | Train Loss: 0.001854 | Validation_Loss: 0.000957 | \n",
      "Model: MLP_AR |Epoch: 131 | Train Loss: 0.001851 | Validation_Loss: 0.000959 | \n",
      "Model: MLP_AR |Epoch: 132 | Train Loss: 0.001850 | Validation_Loss: 0.000957 | \n",
      "Model: MLP_AR |Epoch: 133 | Train Loss: 0.001848 | Validation_Loss: 0.000956 | \n",
      "Model: MLP_AR |Epoch: 134 | Train Loss: 0.001846 | Validation_Loss: 0.000955 | \n",
      "Model: MLP_AR |Epoch: 135 | Train Loss: 0.001844 | Validation_Loss: 0.000954 | \n",
      "Model: MLP_AR |Epoch: 136 | Train Loss: 0.001842 | Validation_Loss: 0.000953 | \n",
      "Model: MLP_AR |Epoch: 137 | Train Loss: 0.001840 | Validation_Loss: 0.000952 | \n",
      "Model: MLP_AR |Epoch: 138 | Train Loss: 0.001838 | Validation_Loss: 0.000951 | \n",
      "Model: MLP_AR |Epoch: 139 | Train Loss: 0.001836 | Validation_Loss: 0.000950 | \n",
      "Model: MLP_AR |Epoch: 140 | Train Loss: 0.001835 | Validation_Loss: 0.000949 | \n",
      "Model: MLP_AR |Epoch: 141 | Train Loss: 0.001833 | Validation_Loss: 0.000948 | \n",
      "Model: MLP_AR |Epoch: 142 | Train Loss: 0.001831 | Validation_Loss: 0.000947 | \n",
      "Model: MLP_AR |Epoch: 143 | Train Loss: 0.001829 | Validation_Loss: 0.000946 | \n",
      "Model: MLP_AR |Epoch: 144 | Train Loss: 0.001828 | Validation_Loss: 0.000945 | \n",
      "Model: MLP_AR |Epoch: 145 | Train Loss: 0.001826 | Validation_Loss: 0.000944 | \n",
      "Model: MLP_AR |Epoch: 146 | Train Loss: 0.001824 | Validation_Loss: 0.000943 | \n",
      "Model: MLP_AR |Epoch: 147 | Train Loss: 0.001823 | Validation_Loss: 0.000943 | \n",
      "Model: MLP_AR |Epoch: 148 | Train Loss: 0.001821 | Validation_Loss: 0.000942 | \n",
      "Model: MLP_AR |Epoch: 149 | Train Loss: 0.001820 | Validation_Loss: 0.000941 | \n",
      "Model: MLP_AR |Epoch: 150 | Train Loss: 0.001818 | Validation_Loss: 0.000940 | \n",
      "Model: MLP_AR |Epoch: 151 | Train Loss: 0.001817 | Validation_Loss: 0.000941 | \n",
      "Model: MLP_AR |Epoch: 152 | Train Loss: 0.001816 | Validation_Loss: 0.000940 | \n",
      "Model: MLP_AR |Epoch: 153 | Train Loss: 0.001814 | Validation_Loss: 0.000940 | \n",
      "Model: MLP_AR |Epoch: 154 | Train Loss: 0.001813 | Validation_Loss: 0.000939 | \n",
      "Model: MLP_AR |Epoch: 155 | Train Loss: 0.001811 | Validation_Loss: 0.000939 | \n",
      "Model: MLP_AR |Epoch: 156 | Train Loss: 0.001810 | Validation_Loss: 0.000938 | \n",
      "Model: MLP_AR |Epoch: 157 | Train Loss: 0.001809 | Validation_Loss: 0.000938 | \n",
      "Model: MLP_AR |Epoch: 158 | Train Loss: 0.001807 | Validation_Loss: 0.000938 | \n",
      "Model: MLP_AR |Epoch: 159 | Train Loss: 0.001806 | Validation_Loss: 0.000937 | \n",
      "Model: MLP_AR |Epoch: 160 | Train Loss: 0.001804 | Validation_Loss: 0.000937 | \n",
      "Model: MLP_AR |Epoch: 161 | Train Loss: 0.001803 | Validation_Loss: 0.000937 | \n",
      "Model: MLP_AR |Epoch: 162 | Train Loss: 0.001802 | Validation_Loss: 0.000936 | \n",
      "Model: MLP_AR |Epoch: 163 | Train Loss: 0.001800 | Validation_Loss: 0.000937 | \n",
      "Model: MLP_AR |Epoch: 164 | Train Loss: 0.001799 | Validation_Loss: 0.000936 | \n",
      "Model: MLP_AR |Epoch: 165 | Train Loss: 0.001798 | Validation_Loss: 0.000935 | \n",
      "Model: MLP_AR |Epoch: 166 | Train Loss: 0.001797 | Validation_Loss: 0.000935 | \n",
      "Model: MLP_AR |Epoch: 167 | Train Loss: 0.001795 | Validation_Loss: 0.000935 | \n",
      "Model: MLP_AR |Epoch: 168 | Train Loss: 0.001794 | Validation_Loss: 0.000934 | \n",
      "Model: MLP_AR |Epoch: 169 | Train Loss: 0.001793 | Validation_Loss: 0.000934 | \n",
      "Model: MLP_AR |Epoch: 170 | Train Loss: 0.001792 | Validation_Loss: 0.000933 | \n",
      "Model: MLP_AR |Epoch: 171 | Train Loss: 0.001790 | Validation_Loss: 0.000932 | \n",
      "Model: MLP_AR |Epoch: 172 | Train Loss: 0.001789 | Validation_Loss: 0.000932 | \n",
      "Model: MLP_AR |Epoch: 173 | Train Loss: 0.001788 | Validation_Loss: 0.000932 | \n",
      "Model: MLP_AR |Epoch: 174 | Train Loss: 0.001787 | Validation_Loss: 0.000932 | \n",
      "Model: MLP_AR |Epoch: 175 | Train Loss: 0.001786 | Validation_Loss: 0.000931 | \n",
      "Model: MLP_AR |Epoch: 176 | Train Loss: 0.001785 | Validation_Loss: 0.000931 | \n",
      "Model: MLP_AR |Epoch: 177 | Train Loss: 0.001784 | Validation_Loss: 0.000931 | \n",
      "Model: MLP_AR |Epoch: 178 | Train Loss: 0.001783 | Validation_Loss: 0.000929 | \n",
      "Model: MLP_AR |Epoch: 179 | Train Loss: 0.001782 | Validation_Loss: 0.000930 | \n",
      "Model: MLP_AR |Epoch: 180 | Train Loss: 0.001781 | Validation_Loss: 0.000929 | \n",
      "Model: MLP_AR |Epoch: 181 | Train Loss: 0.001779 | Validation_Loss: 0.000929 | \n",
      "Model: MLP_AR |Epoch: 182 | Train Loss: 0.001778 | Validation_Loss: 0.000930 | \n",
      "Model: MLP_AR |Epoch: 183 | Train Loss: 0.001777 | Validation_Loss: 0.000929 | \n",
      "Model: MLP_AR |Epoch: 184 | Train Loss: 0.001776 | Validation_Loss: 0.000928 | \n",
      "Model: MLP_AR |Epoch: 185 | Train Loss: 0.001775 | Validation_Loss: 0.000928 | \n",
      "Model: MLP_AR |Epoch: 186 | Train Loss: 0.001774 | Validation_Loss: 0.000928 | \n",
      "Model: MLP_AR |Epoch: 187 | Train Loss: 0.001773 | Validation_Loss: 0.000928 | \n",
      "Model: MLP_AR |Epoch: 188 | Train Loss: 0.001771 | Validation_Loss: 0.000927 | \n",
      "Model: MLP_AR |Epoch: 189 | Train Loss: 0.001770 | Validation_Loss: 0.000927 | \n",
      "Model: MLP_AR |Epoch: 190 | Train Loss: 0.001769 | Validation_Loss: 0.000927 | \n",
      "Model: MLP_AR |Epoch: 191 | Train Loss: 0.001768 | Validation_Loss: 0.000926 | \n",
      "Model: MLP_AR |Epoch: 192 | Train Loss: 0.001768 | Validation_Loss: 0.000926 | \n",
      "Model: MLP_AR |Epoch: 193 | Train Loss: 0.001767 | Validation_Loss: 0.000926 | \n",
      "Model: MLP_AR |Epoch: 194 | Train Loss: 0.001765 | Validation_Loss: 0.000926 | \n",
      "Model: MLP_AR |Epoch: 195 | Train Loss: 0.001765 | Validation_Loss: 0.000926 | \n",
      "Model: MLP_AR |Epoch: 196 | Train Loss: 0.001764 | Validation_Loss: 0.000925 | \n",
      "Model: MLP_AR |Epoch: 197 | Train Loss: 0.001763 | Validation_Loss: 0.000925 | \n",
      "Model: MLP_AR |Epoch: 198 | Train Loss: 0.001762 | Validation_Loss: 0.000925 | \n",
      "Model: MLP_AR |Epoch: 199 | Train Loss: 0.001761 | Validation_Loss: 0.000925 | \n",
      "Model: MLP_AR |Epoch: 200 | Train Loss: 0.001760 | Validation_Loss: 0.000925 | \n",
      "Model: MLP_AR |Epoch: 201 | Train Loss: 0.001759 | Validation_Loss: 0.000924 | \n",
      "Model: MLP_AR |Epoch: 202 | Train Loss: 0.001758 | Validation_Loss: 0.000924 | \n",
      "Model: MLP_AR |Epoch: 203 | Train Loss: 0.001757 | Validation_Loss: 0.000924 | \n",
      "Model: MLP_AR |Epoch: 204 | Train Loss: 0.001756 | Validation_Loss: 0.000923 | \n",
      "Model: MLP_AR |Epoch: 205 | Train Loss: 0.001755 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 206 | Train Loss: 0.001754 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 207 | Train Loss: 0.001753 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 208 | Train Loss: 0.001752 | Validation_Loss: 0.000921 | \n",
      "Model: MLP_AR |Epoch: 209 | Train Loss: 0.001752 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 210 | Train Loss: 0.001751 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 211 | Train Loss: 0.001750 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 212 | Train Loss: 0.001749 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 213 | Train Loss: 0.001748 | Validation_Loss: 0.000922 | \n",
      "Model: MLP_AR |Epoch: 214 | Train Loss: 0.001748 | Validation_Loss: 0.000921 | \n",
      "Model: MLP_AR |Epoch: 215 | Train Loss: 0.001747 | Validation_Loss: 0.000921 | \n",
      "Model: MLP_AR |Epoch: 216 | Train Loss: 0.001746 | Validation_Loss: 0.000921 | \n",
      "Model: MLP_AR |Epoch: 217 | Train Loss: 0.001745 | Validation_Loss: 0.000920 | \n",
      "Model: MLP_AR |Epoch: 218 | Train Loss: 0.001745 | Validation_Loss: 0.000920 | \n",
      "Model: MLP_AR |Epoch: 219 | Train Loss: 0.001744 | Validation_Loss: 0.000918 | \n",
      "Model: MLP_AR |Epoch: 220 | Train Loss: 0.001743 | Validation_Loss: 0.000918 | \n",
      "Model: MLP_AR |Epoch: 221 | Train Loss: 0.001742 | Validation_Loss: 0.000918 | \n",
      "Model: MLP_AR |Epoch: 222 | Train Loss: 0.001741 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 223 | Train Loss: 0.001741 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 224 | Train Loss: 0.001740 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 225 | Train Loss: 0.001739 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 226 | Train Loss: 0.001739 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 227 | Train Loss: 0.001738 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 228 | Train Loss: 0.001737 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 229 | Train Loss: 0.001737 | Validation_Loss: 0.000916 | \n",
      "Model: MLP_AR |Epoch: 230 | Train Loss: 0.001736 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 231 | Train Loss: 0.001735 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 232 | Train Loss: 0.001735 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 233 | Train Loss: 0.001734 | Validation_Loss: 0.000917 | \n",
      "Model: MLP_AR |Epoch: 234 | Train Loss: 0.001733 | Validation_Loss: 0.000916 | \n",
      "Model: MLP_AR |Epoch: 235 | Train Loss: 0.001733 | Validation_Loss: 0.000916 | \n",
      "Model: MLP_AR |Epoch: 236 | Train Loss: 0.001732 | Validation_Loss: 0.000916 | \n",
      "Model: MLP_AR |Epoch: 237 | Train Loss: 0.001731 | Validation_Loss: 0.000920 | \n",
      "Model: MLP_AR |Epoch: 238 | Train Loss: 0.001731 | Validation_Loss: 0.000920 | \n",
      "Model: MLP_AR |Epoch: 239 | Train Loss: 0.001730 | Validation_Loss: 0.000919 | \n",
      "Model: MLP_AR |Epoch: 240 | Train Loss: 0.001729 | Validation_Loss: 0.000920 | \n",
      "Model: MLP_AR |Epoch: 241 | Train Loss: 0.001729 | Validation_Loss: 0.000922 | \n",
      "Early Stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "early_stoppper_ar = EarlyStopping(patience=es_patience, min_delta=0)  \n",
    "train_results_ar = train_autoregressive(model=forward_model, U_scaled_train=U_train_scaled, X_scaled_train=X_train_scaled,\n",
    "                                        U_scaled_valid=U_valid_scaled, X_scaled_valid=X_valid_scaled, lag_input=lag_input, lag_state=lag_state, \n",
    "                                        epochs=epochs_autoregressive, optimizer=optimizer_autoregressive, loss_fn=loss_fn, model_name=model_name, chunk_length=chunk_length , early_stopping_call=early_stoppper_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32d96e",
   "metadata": {},
   "source": [
    "plot of the training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c213056",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_performance_dir_name = \"network_performance\" \n",
    "network_performance_dir_path = os.path.join(script_directory, network_performance_dir_name)\n",
    "os.makedirs(network_performance_dir_path, exist_ok=True)\n",
    "plot_name = \"train_results.png\"\n",
    "plt.figure()\n",
    "plt.plot(train_results_ar[\"valid_loss\"], label=\"valid_loss\")  \n",
    "plt.plot(train_results_ar[\"train_loss\"], label=\"train_loss\")\n",
    "plt.xlabel(\"epochs\") \n",
    "plt.ylabel(\"MSE per epoch [m]\")   \n",
    "plt.title(\"Parallel Training and Validation Losses\")\n",
    "plt.legend()\n",
    "plt.tight_layout() \n",
    "plt.savefig(os.path.join(network_performance_dir_path, plot_name)) \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74c704",
   "metadata": {},
   "source": [
    "plotting teacher forcing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b58d98fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_train_data_loss:  0.0009322594285769088\n",
      "tf_valid_data_loss:  0.0009303452024624487\n",
      "tf_test_data_loss:  0.0008495069644827829\n"
     ]
    }
   ],
   "source": [
    "def teacherforcing_performance_plot(model : torch.nn.Module, dataloader : DataLoader, loss_fn: torch.nn, dataset_name : str) : \n",
    "    labels = [] \n",
    "    preds = [] \n",
    "    model.eval() \n",
    "    total_loss = 0.0\n",
    "    with torch.inference_mode() : \n",
    "        for X , y in dataloader : \n",
    "            # forward pass \n",
    "            pred = model(X) \n",
    "            # appending \n",
    "            labels.append(y) \n",
    "            preds.append(pred) \n",
    "            # calculating the loss \n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss/len(dataloader)\n",
    "    print(f\"{dataset_name}_loss: \", average_loss) \n",
    "\n",
    "    labels = np.concatenate(labels, axis=0) \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    # plot directories \n",
    "    network_tf_performmance_dir_name = \"tf_performance\" \n",
    "    network_tf_performmance_dir_path = os.path.join(network_performance_dir_path, network_tf_performmance_dir_name) \n",
    "    os.makedirs(network_tf_performmance_dir_path, exist_ok=True)\n",
    "\n",
    "    plot_name = f\"{dataset_name}_midx_tf.png\"\n",
    "    plt.figure() \n",
    "    plt.plot(labels[:,0], label='True Mid Section X') \n",
    "    plt.plot(preds[:,0], label='Predicted Mid Section X', linestyle='--')  \n",
    "    plt.legend()\n",
    "    plt.title(\"Mid Section X\"); \n",
    "    plt.xlabel('Sample'); \n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_midy_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(labels[:, 1], label='True Mid Section Y')\n",
    "    plt.plot(preds[:, 1],  label='Predicted Mid Section Y', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Mid Section Y\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_midz_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(labels[:, 2], label='True Mid Section Z')\n",
    "    plt.plot(preds[:, 2],  label='Predicted Mid Section Z', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"Mid Section Z\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_eex_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(labels[:, 3], label='True EE X')\n",
    "    plt.plot(preds[:, 3],  label='Predicted EE X', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"EE X\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_eey_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(labels[:, 4], label='True EE Y')\n",
    "    plt.plot(preds[:, 4],  label='Predicted EE Y', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"EE Y\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_eez_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(labels[:, 5], label='True EE Z')\n",
    "    plt.plot(preds[:, 5],  label='Predicted EE Z', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"EE Z\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()  \n",
    "\n",
    "teacherforcing_performance_plot(model=forward_model, dataloader=train_dataloader, loss_fn=loss_fn, dataset_name=\"tf_train_data\")\n",
    "teacherforcing_performance_plot(model=forward_model, dataloader=valid_dataloader,  loss_fn=loss_fn, dataset_name=\"tf_valid_data\")\n",
    "teacherforcing_performance_plot(model=forward_model, dataloader=test_dataloader,  loss_fn=loss_fn, dataset_name=\"tf_test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b0320",
   "metadata": {},
   "source": [
    "plotting AR results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss of ar_train_data: 0.0008868287550285459\n",
      "Average Loss of ar_valid_data: 0.0009224304230883718\n",
      "Average Loss of ar_test_data: 0.0006932922406122088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006932922406122088"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def autoregressive_performance_plot(model: torch.nn.Module, U_scaled: np.array, X_scaled: np.array,  \n",
    "                                    lag_input: int, lag_state: int, \n",
    "                                    loss_fn: torch.nn.Module, \n",
    "                                    dataset_name: str, folder_name: str, \n",
    "                                    early_stopping_call = None): \n",
    "\n",
    "    model.eval()  \n",
    "\n",
    "    U = torch.from_numpy(U_scaled).type(torch.float32)\n",
    "    X = torch.from_numpy(X_scaled).type(torch.float32)  \n",
    "\n",
    "    X_buffer = X.clone()\n",
    "\n",
    "    max_lag = max(lag_input, lag_state)\n",
    "    \n",
    "    preds = [] \n",
    "\n",
    "    # initial buffer filled with ground truth  \n",
    "    current_state = X[max_lag,:] \n",
    "    if lag_state == 0 : \n",
    "        past_state = X[max_lag:max_lag] \n",
    "    else : \n",
    "        past_state = X[max_lag-lag_state:max_lag,:] \n",
    "        past_state = torch.flatten(input=past_state) \n",
    "    current_input = U[max_lag,:] \n",
    "    if lag_input == 0 : \n",
    "        past_input = U[max_lag:max_lag,:]\n",
    "    else : \n",
    "        past_input = U[max_lag-lag_input:max_lag,:] \n",
    "        past_input = torch.flatten(input=past_input) \n",
    "\n",
    "    if past_state.size(dim=0) == 0 and past_input.size(dim=0) == 0 : \n",
    "        joined_features = torch.concatenate((current_state, current_input), dim=0) \n",
    "    elif past_state.size(dim=0) != 0 and past_input.size(dim=0) == 0 : \n",
    "        joined_features = torch.concatenate((current_state, past_state, current_input), dim=0)\n",
    "    elif past_state.size(dim=0) == 0 and past_input.size(dim=0) != 0 : \n",
    "        joined_features = torch.concatenate((current_state, current_input, past_input), dim=0) \n",
    "    else : \n",
    "        joined_features = torch.concatenate((current_state, past_state, current_input, past_input), dim=0)  \n",
    "\n",
    "    with torch.inference_mode() : \n",
    "\n",
    "        for i in range(max_lag+1, len(U)) : \n",
    "\n",
    "            pred = model(joined_features.unsqueeze(0)) \n",
    "            pred = pred.squeeze(0)\n",
    "\n",
    "            preds.append(pred) \n",
    "\n",
    "            X_buffer[i,:] = pred \n",
    "\n",
    "            if i < len(U) - 1 :\n",
    "                # buffer update \n",
    "                current_state = pred\n",
    "                if lag_state == 0 : \n",
    "                    past_state = X[i:i] \n",
    "                else : \n",
    "                    past_state = X_buffer[i-lag_state:i,:] \n",
    "                    past_state = torch.flatten(input=past_state)\n",
    "                current_input = U[i,:] \n",
    "                if lag_input == 0 : \n",
    "                    past_input = U[i:i,:]\n",
    "                else : \n",
    "                    past_input = U[i-lag_input:i,:] \n",
    "                    past_input = torch.flatten(input=past_input) \n",
    "\n",
    "                if past_state.size(dim=0) == 0 and past_input.size(dim=0) == 0 : \n",
    "                    joined_features = torch.concatenate((current_state, current_input), dim=0) \n",
    "                elif past_state.size(dim=0) != 0 and past_input.size(dim=0) == 0 : \n",
    "                    joined_features = torch.concatenate((current_state, past_state, current_input), dim=0)\n",
    "                elif past_state.size(dim=0) == 0 and past_input.size(dim=0) != 0 : \n",
    "                    joined_features = torch.concatenate((current_state, current_input, past_input), dim=0) \n",
    "                else : \n",
    "                    joined_features = torch.concatenate((current_state, past_state, current_input, past_input), dim=0)\n",
    "            \n",
    "            else : \n",
    "                pass\n",
    "            \n",
    "\n",
    "    preds_tensor = torch.stack(preds, dim=0) \n",
    "\n",
    "    targets = X[max_lag+1:] \n",
    "\n",
    "    loss = loss_fn(preds_tensor, targets)  \n",
    "\n",
    "    average_loss = loss.item()  \n",
    "\n",
    "    print(f\"Average Loss of {dataset_name}: {average_loss}\")\n",
    "\n",
    "    # plot directories \n",
    "    network_tf_performmance_dir_name = folder_name \n",
    "    network_tf_performmance_dir_path = os.path.join(network_performance_dir_path, network_tf_performmance_dir_name) \n",
    "    os.makedirs(network_tf_performmance_dir_path, exist_ok=True)\n",
    "\n",
    "    plot_name = f\"{dataset_name}_midx_tf.png\"\n",
    "    plt.figure() \n",
    "    plt.plot(targets[:,0], label='True Mid Section X') \n",
    "    plt.plot(preds_tensor[:,0], label='Predicted Mid Section X', linestyle='--')  \n",
    "    plt.legend()\n",
    "    plt.title(\"Mid Section X\"); \n",
    "    plt.xlabel('Sample'); \n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_midy_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(targets[:, 1], label='True Mid Section Y')\n",
    "    plt.plot(preds_tensor[:, 1],  label='Predicted Mid Section Y', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Mid Section Y\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_midz_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(targets[:, 2], label='True Mid Section Z')\n",
    "    plt.plot(preds_tensor[:, 2],  label='Predicted Mid Section Z', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"Mid Section Z\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_eex_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(targets[:, 3], label='True EE X')\n",
    "    plt.plot(preds_tensor[:, 3],  label='Predicted EE X', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"EE X\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_eey_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(targets[:, 4], label='True EE Y')\n",
    "    plt.plot(preds_tensor[:, 4],  label='Predicted EE Y', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"EE Y\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]') \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()\n",
    "\n",
    "    plot_name = f\"{dataset_name}_eez_tf.png\"\n",
    "    plt.figure()\n",
    "    plt.plot(targets[:, 5], label='True EE Z')\n",
    "    plt.plot(preds_tensor[:, 5],  label='Predicted EE Z', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(\"EE Z\")\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value [m]')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(network_tf_performmance_dir_path, plot_name)) \n",
    "    plt.close()  \n",
    "\n",
    "    return average_loss \n",
    "\n",
    "autoregressive_performance_plot(model=forward_model, U_scaled=U_train_scaled, X_scaled=X_train_scaled, lag_input=lag_input, lag_state=lag_state,\n",
    "                                loss_fn=loss_fn, dataset_name=\"ar_train_data\", folder_name= \"ar_performance\") \n",
    "\n",
    "autoregressive_performance_plot(model=forward_model, U_scaled=U_valid_scaled, X_scaled=X_valid_scaled, lag_input=lag_input, lag_state=lag_state,\n",
    "                                loss_fn=loss_fn, dataset_name=\"ar_valid_data\", folder_name= \"ar_performance\") \n",
    "\n",
    "autoregressive_performance_plot(model=forward_model, U_scaled=U_test_scaled, X_scaled=X_test_scaled, lag_input=lag_input, lag_state=lag_state,\n",
    "                                loss_fn=loss_fn, dataset_name=\"ar_test_data\", folder_name= \"ar_performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f2133",
   "metadata": {},
   "source": [
    "testing the network with the lines dataset (to assess performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0210c709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataset:  (295, 9)\n",
      "line input shape:  (295, 3)\n",
      "line state shape:  (295, 6)\n",
      "Average Loss of lines_data: 0.0001232621871167794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0001232621871167794"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_dataset = load_data(os.path.join(dataset_path, lines_dataset_filename)) \n",
    "print(\"shape of the dataset: \", lines_dataset.shape)\n",
    "U_lines = lines_dataset[:, input_start_index:input_stop_index]\n",
    "X_lines = lines_dataset[:, state_start_index:state_stop_index]\n",
    "print(\"line input shape: \", U_lines.shape) \n",
    "print(\"line state shape: \", X_lines.shape)\n",
    "U_lines_scaled = input_scaler.transform(U_lines) \n",
    "X_lines_scaled = state_scaler.transform(X_lines) \n",
    "autoregressive_performance_plot(model=forward_model, U_scaled=U_lines_scaled, X_scaled=X_lines_scaled, lag_input=lag_input, lag_state=lag_state, \n",
    "                                loss_fn=loss_fn, dataset_name=\"lines_data\", folder_name= \"lines_performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f91b46",
   "metadata": {},
   "source": [
    "saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0f88b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"forward_MLP.pth\" \n",
    "torch.save(forward_model.state_dict(), os.path.join(network_data_dir_path, model_name)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
